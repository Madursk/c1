{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({\n",
    "  TeX: { equationNumbers: { autoNumber: \"all\" } }\n",
    "});\n",
    "</script>\n",
    "\n",
    "<h1 align=center>Artificial neural networks</h1>\n",
    "<br><br>\n",
    "<center><h4>PhD. Eduardo Ulises Moya</h4></center>\n",
    "<center><h4>Abraham S치nchez</h4></center>\n",
    "<center><h4>PhD. Ulises Cort칠s</h4></center>\n",
    "\n",
    "<center><h4>BSC</h4></center>\n",
    "<center><h4>UAG</h4></center>\n",
    "<center><h4>UPC/BSC</h4></center>\n",
    "<br><br>\n",
    "\n",
    "### Table of Contents:\n",
    "<ul>\n",
    "    <li><p><a href=\"#ref1\">**1. Biological Neuron**</a></p></li>\n",
    "    <ul>\n",
    "        <li><p><a href=\"#ref1_1\">1.1. Elements of a biological neuron</a></p></li>\n",
    "    </ul>\n",
    "    <li><p><a href=\"#ref2\">**2. Artificial Neuron**</a></p></li>\n",
    "    <ul>\n",
    "        <li><p><a href=\"#ref2_1\">2.1. Elements of an artificial neuron</a></p></li>\n",
    "    </ul>\n",
    "    <li><p><a href=\"#ref3\">**3. Model**</a></p></li>\n",
    "    <ul>\n",
    "        <li><p><a href=\"#ref3_1\">3.1. Mathematical model</a></p></li>\n",
    "        <li><p><a href=\"#ref3_2\">3.2. Activation Function</a></p></li>\n",
    "        <li><p><a href=\"#ref3_3\">3.3. Learning stage</a></p></li>\n",
    "    </ul>\n",
    "    <li><p><a href=\"#ref4\">**4. Perceptron**</a></p></li>\n",
    "    <ul>\n",
    "        <li><p><a href=\"#ref4_1\">Implementation</a></p></li>\n",
    "    </ul>\n",
    "    <li><p><a href=\"#ref5\">**5. Backpropagation algorithm**</a></p></li>\n",
    "    <ul>\n",
    "        <li><p><a href=\"#ref5_1\">5.1 Feedforward step</a></p></li>\n",
    "        <li><p><a href=\"#ref5_2\">5.2 Backpropagation step</a></p></li>\n",
    "        <li><p><a href=\"#ref5_3\">Implementation</a></p></li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2>**1. Biological Neuron**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is defined as a population of physically interconnected neurons or a group of isolated neurons that receive signals that process in the manner of a recognizable circuit.\n",
    "\n",
    "The communication between neurons, which implies an electrochemical process, implies that, once a neuron is excited from a certain threshold, it is depolarized by transmitting through its axon a signal that excites nearby neurons, and so on.\n",
    "\n",
    "Neurons have three main components: the dendrites, the body of the cell or soma and the axon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1_1\"></a>\n",
    "<h3>**1.1. Elements of a biological neuron**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dendrites.** Receiving element; dentrites are like nerve fibers that carry electrical signals to the body of the cell; during growth the number of dendrites increases, but then the specialization predominates.\n",
    "- **Soma.** Sum of those signals. It includes the nucleus, it is where energy is mainly produced.\n",
    "- **Ax칩n.** It is a long fiber that carries the signal from the body of the cell to others. The point of contact between the axon of one cell and the dendrite of another is called the synapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "<h2>**2. Artificial Neuron**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model of an artificial neuron is an imitation of the process of a biological neuron. An artificial neuron is the fundamental unit for the operation of an artificial neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2_1\"></a>\n",
    "<h3>**2.1. Elements of an artificial neuron**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the biological model, the following elements are found with the artificial system:\n",
    "\n",
    "- **Input ($X$).** It represents the signals coming from other neurons, captured by the dendrites.\n",
    "- **Weights ($W$).** It represents the intensity of the synapse that connects two neurons. \n",
    "- **Activation Function (Threshold $\\theta$)**. Threshold that the signal of the neuron must surpass to activate (it happens biologically in the body of the cell).\n",
    "- **Outut ($y$).** Output value of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2>**3. Model**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neuron is the fundamental information processing unit in a neural network and its basic model is composed of the following:\n",
    "\n",
    "- **Connection links.** Parameterized by the synaptic weights $W_i$. If $W_i> 0$ then the connection is also exciting if $W_i < 0$ the connection is inhibiting.\n",
    "- **Sum.** Add the components of the input signals multiplied by 洧녥洧녰洧녥洧녰.\n",
    "- **Activation function (Threshold).** Non-linear transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3_1\"></a>\n",
    "<h3>**3.1. Mathematical model**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical terms, it is possible to describe the neuron by the following pair of equations.\n",
    "\n",
    "\\begin{equation}\n",
    "v(k)=\\displaystyle\\sum_{i=1}^{p} x_i(k)w_{i}(k) \\quad o \\quad v(k)=x(k)w^T(k)+b(k)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "y(k)=\\varphi(v(k))\n",
    "\\end{equation}\n",
    "\n",
    "where $洧논_1, 洧논_2, ..., 洧논_洧녴$ are the input signals; $洧녻_1, 洧녻_2, ..., 洧녻_洧녴$ are the synaptic weights; $u$ is the linear combination of the weighted entries; $b$ is the polarization; $\\varphi(.)$ is the activation function; $y$ is the output signal of the neuron; $p$ is the dimension of vector $x$; sub-index $k$ is the iteration for each pattern of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3_2\"></a>\n",
    "<h3>**3.2. Activation Function**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions, denoted by $\\varphi(.)$, Define the output of the neuron. Here are some of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step function</h4>\n",
    "\\begin{equation}\n",
    "\\varphi(v) =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } v >= 0\\\\\n",
    "    0       & \\quad \\text{if } v < 0\n",
    "  \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step function\n",
    "def step(v):\n",
    "    v = v >= 0\n",
    "    return v*1\n",
    "\n",
    "l = np.array(np.arange(-10, 10, 0.1))\n",
    "\n",
    "plt.plot(l, step(l))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sigmoid function</h4>\n",
    "\\begin{equation}\n",
    "\\varphi(v) = \\frac{1}{1+e^-v}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "l = np.array(np.arange(-10, 10, 0.1))\n",
    "f = sigmoid(l)\n",
    "\n",
    "plt.plot(l, sigmoid(l))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU function</h4>\n",
    "\\begin{equation}\n",
    "\\varphi(v) = max(0,v)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def relu(v):\n",
    "    return v*(v>0)\n",
    "\n",
    "l = np.array(np.arange(-10, 10, 0.1))\n",
    "\n",
    "plt.plot(l, relu(l))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3_3\"></a>\n",
    "<h3>**3.3. Learning stage**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning is a process by which the parameters are adapted, by the continuous interaction with the environment. The type of learning is determined by the way in which this adaptation is made.\n",
    "\n",
    "This process involves the following sequence of events:\n",
    "<ul>\n",
    "    <li>The neural network is stimulated by the environment (input data).</li>\n",
    "    <li>The neural network adjusts its parameters (pessos).</li>\n",
    "    <li>The neural network generates a response (activation function).</li>\n",
    "    <li>Stop until the error is the minimum acceptable (i.e. 0.01) or in the case of the perceotron, that all its outputs are equal to those desired.</li>\n",
    "</ul>\n",
    "The weights of the neural network of the figure are adjusted under the following mechanism:\n",
    "\n",
    "\\begin{equation}\n",
    "w(k+1) = w(k) + \\Delta w\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w(k+1) = w(k) + e(k)x(k)\\eta\n",
    "\\end{equation}\n",
    "\n",
    "Where $w$ is the weight matrix; $e$ is the error; $x$ is the entries matrix; $\\eta$ is the learning rate.\n",
    "\n",
    "For the bias:\n",
    "\n",
    "\\begin{equation}\n",
    "b(k+1) = b(k) + \\Delta b\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b(k+1) = b(k) + e(k)\\eta\n",
    "\\end{equation}\n",
    "\n",
    "This mechanism is also known as <b>adaptive filter</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4\"></a>\n",
    "<h2>**4. Perceptron**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is the simplest form of a neural network used for the classification of <b>linearly separable patterns</b> (patterns that are located on opposite sides of a hyperplane).\n",
    "\n",
    "It basically consists of a single neuron with adjustable weights and a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4_1\"></a>\n",
    "<h2>**Implementation**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a perceptron type artificial neuron network which aims to find a solution to an AND logical gate, that is, weights must be found that can classify the points of said gate linearly.\n",
    "\n",
    "Input data:\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    " \\begin{pmatrix}\n",
    "  0 & 0\\\\\n",
    "  0 & 1\\\\\n",
    "  1 & 0\\\\\n",
    "  1 & 1\n",
    " \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Target:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \n",
    " \\begin{pmatrix}\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  1\n",
    " \\end{pmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np # Numeric library python\n",
    "import matplotlib.pyplot as plt # Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron class\n",
    "class SinglePerceptron:\n",
    "\n",
    "  # Constructor\n",
    "  def __init__(self, x=np.array([]), t=np.array([])):\n",
    "    self.x = x\n",
    "    self.t = t\n",
    "    # Init weights\n",
    "    self.w = np.random.rand(len(t[1,:]), len(x[1,:]))\n",
    "    self.b = np.random.rand(1, np.size(t[1,:])) # bias\n",
    "\n",
    "  # Activation function (step)\n",
    "  def step(self, v):\n",
    "    v = v >= 0\n",
    "    return v*1\n",
    "\n",
    "  # Get weights\n",
    "  def weight(self):\n",
    "    return self.w\n",
    "\n",
    "  # Get bias\n",
    "  def bias(self):\n",
    "    return self.b\n",
    "\n",
    "  # Training\n",
    "  def train(self, eta=0.03, max_epoch=100000):\n",
    "    flag = False\n",
    "    epoch = 0\n",
    "    while not flag and epoch < max_epoch:\n",
    "      flag = True\n",
    "      for k in range(0, len(self.x[:,1])):\n",
    "        v = np.dot(self.x[k,:], self.w.T) + self.b\n",
    "        y = self.step(v)\n",
    "        e = self.t[k] - y\n",
    "        if y != self.t[k]:\n",
    "          self.w = self.w + (e * self.x[k,:] * eta)\n",
    "          self.b = self.b + (e * eta)\n",
    "          flag = False\n",
    "      epoch += 1\n",
    "\n",
    "  # Plot results\n",
    "  def plot(self):\n",
    "    if len(self.x[1,:]) > 2 and len(self.t[1,:]) > 2: return\n",
    "    clas = set(self.t.flatten())\n",
    "    colors = ['ro', 'bo', 'yo','go']\n",
    "    rel = dict(zip(clas, colors))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title('Orignal representation')\n",
    "    for n in range(0, len(x[:,1])):\n",
    "      plt.plot(self.x[n,0], self.x[n,1], rel.get(self.t[n][0]))\n",
    "    # Get two point (P1(x1,y1), P2(x2,y2)) and plot line in the hyperplane\n",
    "    # Ax + By + C = 0\n",
    "    A = self.w[0,0]\n",
    "    B = self.w[0,1]\n",
    "    C = self.b[0,0]\n",
    "    x1 = np.max(self.x)\n",
    "    x2 = np.min(self.x)\n",
    "    # In order to find one of the point we have:\n",
    "    y1 = (-C-A*x1)/B\n",
    "    y2 = (-C-A*x2)/B\n",
    "\n",
    "    # Display line\n",
    "    plt.subplot(122)\n",
    "    plt.title('Linear separation')\n",
    "    plt.plot([x1, x2],[y1, y2], '--')\n",
    "    for n in range(0, len(x[:,1])):\n",
    "      plt.plot(self.x[n,0], self.x[n,1], rel.get(self.t[n][0]))\n",
    "    plt.show()\n",
    "\n",
    "  # Simulation\n",
    "  def sim(self, z, noise=0):\n",
    "    v = np.dot(z+noise, self.w.T) + self.b\n",
    "    y = self.step(v)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "x = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "# AND gate representation\n",
    "t = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the perceptron\n",
    "net=SinglePerceptron(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show line before training\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "# We can set learning rate and\n",
    "# the max number of iteration (just in case)\n",
    "\n",
    "net.train(eta=0.05) # default eta value as 0.03\n",
    "\n",
    "# Show training data\n",
    "print (\"\\\"Statistics\\\"\")\n",
    "print (\"Weights after training..\\n\")\n",
    "print (\"W:\\n\", net.weight())\n",
    "print (\"b:\\n\", net.bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "# We are going to try with the same target data so...\n",
    "\n",
    "res = net.sim(x)\n",
    "\n",
    "print (\"\\n\\\"Simulation\\\"\")\n",
    "print (\"Target:\\n\", t)\n",
    "print (\"\\nResult:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification, after training\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "x = np.array([[0.5,0.5],\n",
    "             [0.3,0.1],\n",
    "             [0.1,-0.1],\n",
    "             [-0.2,0.5],\n",
    "             [0.4,-1.0],\n",
    "             [-0.5,0.5],\n",
    "             [0.5,-0.5],\n",
    "             [0.0,-0.5],\n",
    "             [0.5,0.0],\n",
    "             [0.0,0.0],\n",
    "             [0.25,-0.5],\n",
    "             [0.25,0.5],\n",
    "             [1.0,0.5],\n",
    "             [1.5,-0.5],\n",
    "             [1.25,-0.5],\n",
    "             [1.0,-1.0],\n",
    "             [2.0,0.0],\n",
    "             [1.5,0.0]])\n",
    "# Target data \n",
    "t = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the perceptron\n",
    "net=SinglePerceptron(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show line before training\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "# We can set learning rate and\n",
    "# the max number of iteration (just in case)\n",
    "\n",
    "net.train(eta=0.05) # default eta value as 0.03\n",
    "\n",
    "# Show training data\n",
    "print (\"\\\"Statistics\\\"\")\n",
    "print (\"Weights after training..\\n\")\n",
    "print (\"W:\\n\", net.weight())\n",
    "print (\"b:\\n\", net.bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "# We are going to try with the same target data so...\n",
    "\n",
    "res = net.sim(x)\n",
    "\n",
    "print (\"\\n\\\"Simulation\\\"\")\n",
    "print (\"Target:\\n\", t)\n",
    "print (\"\\nResult:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification, after training\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref5\"></a>\n",
    "<h2>**5. Backpropagation**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1969 Minsky and Papert published their book Perceptrons: **\"An introduction to computational geometry\"**, which signified a great stagnation in the theory of neural networks. In it, a detailed analysis of the Perceptron was presented, in terms of its capabilities and limitations, especially in terms of the restrictions that exist for the problems that a Perceptron-type network can solve. The biggest disadvantage of this type of network is its **inability to solve classification problems that are not linearly separable**.\n",
    "\n",
    "The biggest disadvantage of this type of network is its **inability to solve classification problems that are not linearly separable**.\n",
    "\n",
    "The multilayer Perceptron, initially developed by P. Werbos (1974), allows us to solve this problem. It has a structure with at least one hidden layer; and its training algorithm is of the error correction type. Implement the gradient distributed in the different components of the network.\n",
    "\n",
    "Implement the gradient distributed in the different components of the network.\n",
    "\n",
    "**M**ulti**L**ayer **P**erceptron (**MLP**), have been successfully applied to solve very diverse and difficult problems by means of the algorithm known as backpropagation.\n",
    "\n",
    "The backpropagation algorithm consists of two stages:\n",
    "1. **Feedforward**\n",
    "Fixed network parameters. The input signal to the network is presented, which propagates forward to produce the output.\n",
    "2. **Backpropagation**\n",
    "The error between the desired output and the network propagates backwards. The parameters of the network are modified to minimize the square of said error.\n",
    "\n",
    "The MLP has three distinctive characteristics:\n",
    "<ul>\n",
    "<li> <i>The model of each neuron in the network includes a non-linear activation function </i>. The important thing here is that the non-linearity is smooth (differentiable at any point), the opposite of the sign function used in the Perceptron. Logistics is one of the most used activation functions. </li>\n",
    "\n",
    "<li><i>The network contains one or more hidden layers that are not part of the network's inputs or outputs </i>. These hidden neurons allow the network to learn complex tasks by the progressive extraction of the main characteristics of the input patterns.</li>\n",
    "\n",
    "<li><i>The network presents high degrees of connectivity, determined by the synapses of the network </i>. The combination of these characteristics together with the ability to learn from experience through MLP training results in a\n",
    "great computing potential. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref5_1\"></a>\n",
    "<h3>**5.1 Feedforward**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron of each layer is calculated: output of the hidden layer and output of the output layer.\n",
    "\n",
    "Hidden layer:\n",
    "\n",
    "\\begin{equation}\n",
    "v_h(k)=\\displaystyle\\sum_{i=1}^{p} x_i(k)w_{h,i}(k) \\quad o \\quad v_h(k)=x(k)w_h^T(k)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "y_h(k)=\\varphi(v_h(k))\n",
    "\\end{equation}\n",
    "\n",
    "Output layer:\n",
    "\n",
    "The same calculations are made to obtain the outputs of the output neurons.\n",
    "\n",
    "\\begin{equation}\n",
    "v_o(k)=\\displaystyle\\sum_{j=1}^{q} y_{h,j}(k)w_{o,j}(k) \\quad o \\quad v_o(k)=y_h(k)w_o^T(k)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "y_o(k)=\\varphi(v_o(k))\n",
    "\\end{equation}\n",
    "\n",
    "The $h$ sub-index represents the hidden layer; sub-index $o$ is the output layer; $p$ is the dimension or size of vector $x$; $q$ is the number of neurons in the hidden layer; sub-index $k$ is the iteration for each pattern of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref5_2\"></a>\n",
    "<h3>**5.2 Backpropagation**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that to obtain the error a difference is made between the desired value $t(k)$ and the value obtained $y_o(k)$, the purpose of this step is to propagate this error towards each of the layers previous to reach the first layer, this because the intermediate layers (hidden layers) are unaware of the value of the error, this only knows the final layer since this is the one that compares its output against the desired value.\n",
    "\n",
    "In this context, two different cases can be identified, depending on the location of the neuron:\n",
    "\n",
    "**Case 1.** The neuron is an output node. Since the neuron is at the output then it is possible to determine the value of e(k) as and consequently the value of the local gradient is determined directly from the equation.\n",
    "\n",
    "**Case 2.** The neuron is a hidden neuron. When the neuron is located in any of the hidden layers, the desired value for the output of said neuron is not available. Therefore, the error signal of said neuron must be determined recursively in terms of the error signal of the output neurons.\n",
    "\n",
    "For this presentation we will not go into how to obtain the weight update, it will only show the equations necessary to do so.\n",
    "\n",
    "Update weights for the output layer:\n",
    "\n",
    "\\begin{equation}\n",
    "W_o(k+1)=W_o(k)+\\Delta W_o(k)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "W_o(k+1)=W_o(k)+\\eta \\delta(k)y_h(k)\n",
    "\\end{equation}\n",
    "\n",
    "Update weights for the hidden layer:\n",
    "\n",
    "\\begin{equation}\n",
    "W_h(k+1)=W_h(k)+\\Delta W_h(k)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "W_h(k+1)=W_h(k)+\\eta x(k)\\varphi '(v_h(k)) \\displaystyle\\sum \\delta(k) W_o(k)\n",
    "\\end{equation}\n",
    "\n",
    "Like the perceptron, these equations will allow updating the weights in relation to the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref5_3\"></a>\n",
    "<h3>**Implementation**</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem known and known to the community as a non-linearly separable problem is the resolution of the XOR logic gate, which is represented as follows:\n",
    "\n",
    "Input data:\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    " \\begin{pmatrix}\n",
    "  0 & 0\\\\\n",
    "  0 & 1\\\\\n",
    "  1 & 0\\\\\n",
    "  1 & 1\n",
    " \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Target:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \n",
    " \\begin{pmatrix}\n",
    "  0\\\\\n",
    "  1\\\\\n",
    "  1\\\\\n",
    "  0\n",
    " \\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ylim([-0.5,1.5])\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.plot(0,0,'bo')\n",
    "plt.plot(0,1,'ro')\n",
    "plt.plot(1,0,'ro')\n",
    "plt.plot(1,1,'bo')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the red points are class 1 and the blue points are class two. The objective is to use a line that allows linearly separate one class from the other in a 2D plane, impossible, right?. Because a single neuron (for example, considering the perceptron) can draw a single straight line, it is a virtually impossible task. To solve this problem, it is required that more than one line be drawn in order to classify correctly; In spite of continuing to draw straight lines it is possible that using more than one classification is achieved. For example, look at the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ylim([-0.5,1.5])\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.plot(0,0,'bo')\n",
    "plt.plot(0,1,'ro')\n",
    "plt.plot(1,0,'ro')\n",
    "plt.plot(1,1,'bo')\n",
    "plt.plot([-0.5,1],[0.75,-0.5],'--')\n",
    "plt.plot([0.25,1.5],[1.5,0.25],'--')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation in Python language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "\n",
    "  # Constructor\n",
    "  # x - Input matrix\n",
    "  # t - Target matrix\n",
    "  # neurons_hidden - Number of neurons in the hidden layer\n",
    "  def __init__(self, x=np.array([]), t=np.array([]), neurons_hidden=2):\n",
    "    self.x = x\n",
    "    self.t = t\n",
    "    # Init weights\n",
    "    # Hidden layer weights\n",
    "    self.wh = np.random.rand(neurons_hidden, np.size(self.x[1,:]))\n",
    "    # Output layer weights\n",
    "    self.wo = np.random.rand(1, neurons_hidden) # Using 1 hidden layer\n",
    "\n",
    "  # Define activation function\n",
    "  # Activation Function, SIGMOIDE\n",
    "  def sigmoid(self, z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "  # Activation Function, DERIVATIVE SIGMOIDE\n",
    "  def d_sigmoid(self, z):\n",
    "    ds = z * (1 - z)\n",
    "    return ds\n",
    "\n",
    "  # Get weights\n",
    "  def weight(self):\n",
    "    return [self.wh, self.wo]\n",
    "\n",
    "  # Training\n",
    "  def train(self, eta=0.03, min_error=0.01):\n",
    "    av_e = 1\n",
    "    loss = []\n",
    "    epoch = 0\n",
    "    print (\"Training ... this could take a while, depends of the number of neurons established in the hidden layer,\",\n",
    "           \"the learning rate and the error.\\n\")\n",
    "    while av_e > min_error:\n",
    "      E = np.zeros(np.shape(self.t))\n",
    "      for k in range(0, len(self.x[:,1])):\n",
    "        vh = np.dot(self.x[k,:], self.wh.T)\n",
    "        yh = self.sigmoid(vh)\n",
    "        vo = np.dot(yh, self.wo.T)\n",
    "        yo = self.sigmoid(vo)\n",
    "        e = self.t[k] - yo\n",
    "        eo = self.d_sigmoid(yo) * e\n",
    "        eh = np.multiply(self.d_sigmoid(yh), eo * self.wo)\n",
    "        self.wo = self.wo + (eo * yh * eta)\n",
    "        self.wh = self.wh + (eh.T * self.x[k,:] * eta)\n",
    "        E[k] = e**2\n",
    "      av_e = np.sum(E)/len(self.x[:,1])\n",
    "      loss.append(av_e)\n",
    "      epoch += 1\n",
    "    return loss\n",
    "\n",
    "  def plot_loss(self, loss):\n",
    "    plt.plot(loss)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "  # Simulation\n",
    "  def sim(self, z, noise=0.0):\n",
    "    vh = np.dot(z+noise, self.wh.T)\n",
    "    yh = self.sigmoid(vh)\n",
    "    vo = np.dot(yh, self.wo.T)\n",
    "    yo = self.sigmoid(vo)\n",
    "    return yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XOR gate\n",
    "x = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "t = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP\n",
    "# Using default values. 3 neurons in hidden layer\n",
    "net = MultilayerPerceptron(x,t,neurons_hidden=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets simulate XOR with current weights\n",
    "s = net.sim(x)\n",
    "print (s)\n",
    "print (\"\\nIt does not look like a XOR, right?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "# Using default values. Learning rate (eta) as 0.03 and min error as 0.01\n",
    "loss = net.train()\n",
    "\n",
    "# Show training data\n",
    "print (\"\\\"Statistics\\\"\")\n",
    "print (\"Weights after training..\\n\")\n",
    "print (\"W:\\n\", net.weight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate again with adapted weights\n",
    "print (\"\\n\\\"Simulation\\\"\\n\")\n",
    "print (\"Target:\\n\", t)\n",
    "print (\"\\nResult:\")\n",
    "# Simulation\n",
    "res = net.sim(x)\n",
    "print (res)\n",
    "print (\"\\nClose to what we are looking for.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "net.plot_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to try to classify some words like: A, E, I, O, U\n",
    "# So we need to define the digits as a matrix, for example:\n",
    "\n",
    "# A\n",
    "# 11111\n",
    "# 10001\n",
    "# 11111\n",
    "# 10001\n",
    "# 10001\n",
    "\n",
    "# We need to transform this matrix as a vector then:\n",
    "# 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1\n",
    "\n",
    "# Lets do it in code\n",
    "\n",
    "# Define A\n",
    "A = np.array([[1,1,1,1,1],\n",
    "              [1,0,0,0,1],\n",
    "              [1,1,1,1,1],\n",
    "              [1,0,0,0,1],\n",
    "              [1,0,0,0,1]])\n",
    "# Define E\n",
    "E = np.array([[1,1,1,1,1],\n",
    "              [1,0,0,0,0],\n",
    "              [1,1,1,1,1],\n",
    "              [1,0,0,0,0],\n",
    "              [1,1,1,1,1]])\n",
    "# Define I\n",
    "I = np.array([[0,1,1,1,0],\n",
    "              [0,0,1,0,0],\n",
    "              [0,0,1,0,0],\n",
    "              [0,0,1,0,0],\n",
    "              [0,1,1,1,0]])\n",
    "# Define O\n",
    "O = np.array([[0,1,1,1,0],\n",
    "              [1,0,0,0,1],\n",
    "              [1,0,0,0,1],\n",
    "              [1,0,0,0,1],\n",
    "              [0,1,1,1,0]])\n",
    "# Define U\n",
    "U = np.array([[1,0,0,1,0],\n",
    "              [1,0,0,1,0],\n",
    "              [1,0,0,1,0],\n",
    "              [1,0,0,1,0],\n",
    "              [1,1,1,1,0]])\n",
    "\n",
    "# Create Input matrix\n",
    "# In order to create the input matrix we need to convert them as vector\n",
    "# from 5x5 to 1x25\n",
    "A = np.reshape(A, len(A)**2)\n",
    "E = np.reshape(E, len(E)**2)\n",
    "I = np.reshape(I, len(I)**2)\n",
    "O = np.reshape(O, len(O)**2)\n",
    "U = np.reshape(U, len(U)**2)\n",
    "\n",
    "x = np.array([A,E,I,O,U])\n",
    "print (\"Input matrix:\\n\",x)\n",
    "\n",
    "# We are going to define the target value for words above:\n",
    "# A = 0.0\n",
    "# E = 0.2\n",
    "# I = 0.4\n",
    "# O = 0.6\n",
    "# U = 0.8\n",
    "\n",
    "t = np.array([[0.0],\n",
    "              [0.2],\n",
    "              [0.4],\n",
    "              [0.6],\n",
    "              [0.8]])\n",
    "print (\"Target:\\n\",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with 5 neurons in hidden layer\n",
    "net = MultilayerPerceptron(x,t,neurons_hidden=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "# Using default values. Learning rate (eta) as 0.03 and min error as 0.01\n",
    "loss = net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets simulate with word above (i.e A)\n",
    "s = net.sim(A)\n",
    "print (s,\"~ 0 ? then it's close to A\")\n",
    "s = net.sim(E)\n",
    "print (s,\"~ 0.2 ? that it's close to E\")\n",
    "print (\"and so on...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loss function for this training\n",
    "net.plot_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about noisy input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, E is close to 0.2 ...\n",
    "s = net.sim(E,noise=0.01)\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
